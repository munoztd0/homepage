[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Talks\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nDeploy Your R Code!\n\n\n\n\n\n\n\nTalk\n\n\nThoughts\n\n\nWeb\n\n\nR\n\n\nImplementing an idea\n\n\nDevOps\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nDavid Munoz Tord\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Data_manip_Fundamental/index.html",
    "href": "blog/Data_manip_Fundamental/index.html",
    "title": "Data Manipulation with R",
    "section": "",
    "text": "By David Munoz Tord\nGoal: Learn more about data manipulation: how to pivot, join and filter data using {dplyr} and {tidyr} packages.\nOpen it full\nNote: All in english"
  },
  {
    "objectID": "blog/Live/index.html",
    "href": "blog/Live/index.html",
    "title": "We-Data Live on YouTube",
    "section": "",
    "text": "By David Munoz Tord, Fabrice Hategekimana and Vestin Hategekimana\nLive of December 29 edited in which we present in more detail WeData, its functioning and its future.\nNote: video in french, ask in comments for subtitle in your language\nVideo link"
  },
  {
    "objectID": "blog/Data_explor_Fundamentals/index.html",
    "href": "blog/Data_explor_Fundamentals/index.html",
    "title": "Data Exploration with R",
    "section": "",
    "text": "By David Munoz Tord\nGoal: Learn about data exploration and familiarize yourself with some of the basic functions of the tidyverse.\nOpen it full\nNote: All in english"
  },
  {
    "objectID": "blog/llm2/index.html",
    "href": "blog/llm2/index.html",
    "title": "Your first chat bot with python!",
    "section": "",
    "text": "By David Munoz Tord\nGoal: Learn about language models and familiarize yourself with some of the basic functions of the {languagemodels} to create your own chat bot.\n{languagemodels} is a python module designed to be as simple as possible for learners and educators exploring how large language models intersect with modern software development. The interfaces to this package are all simple functions using standard types. The complexity of large language models is hidden from view while providing free local inference using light-weight, open models. All included models are free for educational use, no API keys are required, and all inference is performed locally by default.\nRead more about it\n\nimport languagemodels as lm\n\n\nlm.do(\"What color is the sky?\")\n\n'The color of the sky is blue.'\n\n\ntokenizer.json:   0%|          | 0.00/2.42M [00:00&lt;?, ?B/s]\ntokenizer.json: 100%|##########| 2.42M/2.42M [00:00&lt;00:00, 19.2MB/s]\ntokenizer.json: 100%|##########| 2.42M/2.42M [00:00&lt;00:00, 18.9MB/s]\n\nFetching 6 files:   0%|          | 0/6 [00:00&lt;?, ?it/s]\n\nconfig.json:   0%|          | 0.00/162 [00:00&lt;?, ?B/s]\u001b[A\nconfig.json: 100%|##########| 162/162 [00:00&lt;00:00, 973kB/s]\n\nFetching 6 files:  17%|#6        | 1/6 [00:00&lt;00:01,  3.86it/s]\n\nmodel.bin:   0%|          | 0.00/249M [00:00&lt;?, ?B/s]\u001b[A\n\nmodel.bin:   4%|4         | 10.5M/249M [00:00&lt;00:13, 18.2MB/s]\u001b[A\n\nmodel.bin:   8%|8         | 21.0M/249M [00:00&lt;00:07, 31.3MB/s]\u001b[A\n\nmodel.bin:  13%|#2        | 31.5M/249M [00:00&lt;00:05, 43.1MB/s]\u001b[A\n\nmodel.bin:  17%|#6        | 41.9M/249M [00:01&lt;00:04, 47.8MB/s]\u001b[A\n\nmodel.bin:  21%|##1       | 52.4M/249M [00:01&lt;00:03, 50.6MB/s]\u001b[A\n\nmodel.bin:  25%|##5       | 62.9M/249M [00:01&lt;00:03, 57.6MB/s]\u001b[A\n\nmodel.bin:  29%|##9       | 73.4M/249M [00:01&lt;00:03, 56.5MB/s]\u001b[A\n\nmodel.bin:  34%|###3      | 83.9M/249M [00:01&lt;00:03, 54.9MB/s]\u001b[A\n\nmodel.bin:  38%|###7      | 94.4M/249M [00:01&lt;00:02, 54.2MB/s]\u001b[A\n\nmodel.bin:  42%|####2     | 105M/249M [00:02&lt;00:02, 55.3MB/s] \u001b[A\n\nmodel.bin:  46%|####6     | 115M/249M [00:02&lt;00:02, 54.5MB/s]\u001b[A\n\nmodel.bin:  51%|#####     | 126M/249M [00:02&lt;00:02, 55.0MB/s]\u001b[A\n\nmodel.bin:  55%|#####4    | 136M/249M [00:02&lt;00:01, 56.7MB/s]\u001b[A\n\nmodel.bin:  59%|#####8    | 147M/249M [00:02&lt;00:01, 60.1MB/s]\u001b[A\n\nmodel.bin:  63%|######3   | 157M/249M [00:03&lt;00:01, 60.7MB/s]\u001b[A\n\nmodel.bin:  67%|######7   | 168M/249M [00:03&lt;00:01, 62.6MB/s]\u001b[A\n\nmodel.bin:  72%|#######1  | 178M/249M [00:03&lt;00:01, 61.4MB/s]\u001b[A\n\nmodel.bin:  76%|#######5  | 189M/249M [00:03&lt;00:01, 59.7MB/s]\u001b[A\n\nmodel.bin:  80%|########  | 199M/249M [00:03&lt;00:01, 49.0MB/s]\u001b[A\n\nmodel.bin:  84%|########4 | 210M/249M [00:04&lt;00:00, 50.5MB/s]\u001b[A\n\nmodel.bin:  88%|########8 | 220M/249M [00:04&lt;00:00, 53.1MB/s]\u001b[A\n\nmodel.bin:  93%|#########2| 231M/249M [00:04&lt;00:00, 57.7MB/s]\u001b[A\n\nmodel.bin:  97%|#########6| 241M/249M [00:04&lt;00:00, 57.1MB/s]\u001b[A\n\nmodel.bin: 100%|##########| 249M/249M [00:04&lt;00:00, 58.1MB/s]\u001b[A\nmodel.bin: 100%|##########| 249M/249M [00:04&lt;00:00, 53.1MB/s]\n\nFetching 6 files:  33%|###3      | 2/6 [00:05&lt;00:13,  3.29s/it]\n\nshared_vocabulary.txt:   0%|          | 0.00/299k [00:00&lt;?, ?B/s]\u001b[A\n\nshared_vocabulary.txt: 100%|##########| 299k/299k [00:00&lt;00:00, 1.54MB/s]\u001b[A\nshared_vocabulary.txt: 100%|##########| 299k/299k [00:00&lt;00:00, 1.54MB/s]\n\nFetching 6 files:  50%|#####     | 3/6 [00:06&lt;00:06,  2.02s/it]\n\nspecial_tokens_map.json:   0%|          | 0.00/2.20k [00:00&lt;?, ?B/s]\u001b[A\nspecial_tokens_map.json: 100%|##########| 2.20k/2.20k [00:00&lt;00:00, 13.3MB/s]\n\nFetching 6 files:  67%|######6   | 4/6 [00:06&lt;00:02,  1.29s/it]\n\ntokenizer_config.json:   0%|          | 0.00/2.50k [00:00&lt;?, ?B/s]\u001b[A\ntokenizer_config.json: 100%|##########| 2.50k/2.50k [00:00&lt;00:00, 14.4MB/s]\n\nFetching 6 files: 100%|##########| 6/6 [00:06&lt;00:00,  1.53it/s]\nFetching 6 files: 100%|##########| 6/6 [00:06&lt;00:00,  1.09s/it]\n\n\n\nTo easy, let’s try something a bit harder\n\nlm.do(\"If I have 7 apples then eat 5, how many apples do I have?\")\n\n'You have 8 apples.'\n\n\nAouch…\nIndeed the model performance is quite low because the models used by this package are 1000x smaller than the largest models in use today. They are useful as learning tools, but if you are expecting ChatGPT or similar performance, you will be very disappointed…\nThe base model should work on any system with 512MB of memory, but this memory limit can be increased. Setting this value higher will require more memory and generate results more slowly, but the results should be superior.\n\nlm.set_max_ram('4gb')\n\n4.0\n\n\n\nlm.do(\"If I have 7 apples then eat 5, how many apples do I have?\")\n\n'After eating 5 apples, you will have 7 - 5 = 4 apples left.'\n\n\ntokenizer.json:   0%|          | 0.00/2.42M [00:00&lt;?, ?B/s]\ntokenizer.json: 100%|##########| 2.42M/2.42M [00:00&lt;00:00, 6.15MB/s]\ntokenizer.json: 100%|##########| 2.42M/2.42M [00:00&lt;00:00, 6.12MB/s]\n\nFetching 7 files:   0%|          | 0/7 [00:00&lt;?, ?it/s]\n\nconfig.json:   0%|          | 0.00/162 [00:00&lt;?, ?B/s]\u001b[A\nconfig.json: 100%|##########| 162/162 [00:00&lt;00:00, 1.01MB/s]\n\nFetching 7 files:  14%|#4        | 1/7 [00:00&lt;00:01,  3.80it/s]\n\ngeneration_config.json:   0%|          | 0.00/142 [00:00&lt;?, ?B/s]\u001b[A\ngeneration_config.json: 100%|##########| 142/142 [00:00&lt;00:00, 842kB/s]\n\nFetching 7 files:  29%|##8       | 2/7 [00:00&lt;00:01,  4.01it/s]\n\nmodel.bin:   0%|          | 0.00/2.86G [00:00&lt;?, ?B/s]\u001b[A\n\nmodel.bin:   0%|          | 10.5M/2.86G [00:00&lt;02:54, 16.3MB/s]\u001b[A\n\nmodel.bin:   1%|          | 21.0M/2.86G [00:00&lt;01:42, 27.7MB/s]\u001b[A\n\nmodel.bin:   1%|1         | 31.5M/2.86G [00:00&lt;01:13, 38.4MB/s]\u001b[A\n\nmodel.bin:   1%|1         | 41.9M/2.86G [00:01&lt;00:59, 47.1MB/s]\u001b[A\n\nmodel.bin:   2%|1         | 52.4M/2.86G [00:01&lt;00:55, 50.2MB/s]\u001b[A\n\nmodel.bin:   2%|2         | 62.9M/2.86G [00:01&lt;00:50, 55.5MB/s]\u001b[A\n\nmodel.bin:   3%|2         | 73.4M/2.86G [00:01&lt;00:48, 57.6MB/s]\u001b[A\n\nmodel.bin:   3%|2         | 83.9M/2.86G [00:01&lt;00:58, 47.7MB/s]\u001b[A\n\nmodel.bin:   3%|3         | 94.4M/2.86G [00:02&lt;00:55, 50.0MB/s]\u001b[A\n\nmodel.bin:   4%|3         | 105M/2.86G [00:02&lt;00:59, 45.9MB/s] \u001b[A\n\nmodel.bin:   4%|4         | 115M/2.86G [00:02&lt;01:02, 44.2MB/s]\u001b[A\n\nmodel.bin:   4%|4         | 126M/2.86G [00:02&lt;00:57, 47.4MB/s]\u001b[A\n\nmodel.bin:   5%|4         | 136M/2.86G [00:03&lt;01:06, 40.7MB/s]\u001b[A\n\nmodel.bin:   5%|5         | 147M/2.86G [00:03&lt;01:03, 42.4MB/s]\u001b[A\n\nmodel.bin:   6%|5         | 157M/2.86G [00:03&lt;00:58, 46.3MB/s]\u001b[A\n\nmodel.bin:   6%|5         | 168M/2.86G [00:03&lt;00:55, 48.2MB/s]\u001b[A\n\nmodel.bin:   6%|6         | 178M/2.86G [00:03&lt;00:49, 53.9MB/s]\u001b[A\n\nmodel.bin:   7%|6         | 189M/2.86G [00:04&lt;00:48, 54.9MB/s]\u001b[A\n\nmodel.bin:   7%|6         | 199M/2.86G [00:04&lt;00:47, 55.4MB/s]\u001b[A\n\nmodel.bin:   7%|7         | 210M/2.86G [00:04&lt;00:49, 53.5MB/s]\u001b[A\n\nmodel.bin:   8%|7         | 220M/2.86G [00:04&lt;00:48, 54.3MB/s]\u001b[A\n\nmodel.bin:   8%|8         | 231M/2.86G [00:04&lt;00:44, 59.1MB/s]\u001b[A\n\nmodel.bin:   8%|8         | 241M/2.86G [00:04&lt;00:44, 58.9MB/s]\u001b[A\n\nmodel.bin:   9%|8         | 252M/2.86G [00:05&lt;00:41, 63.4MB/s]\u001b[A\n\nmodel.bin:   9%|9         | 262M/2.86G [00:05&lt;00:45, 57.1MB/s]\u001b[A\n\nmodel.bin:  10%|9         | 273M/2.86G [00:05&lt;00:48, 53.5MB/s]\u001b[A\n\nmodel.bin:  10%|9         | 283M/2.86G [00:05&lt;00:48, 52.7MB/s]\u001b[A\n\nmodel.bin:  10%|#         | 294M/2.86G [00:06&lt;00:53, 47.9MB/s]\u001b[A\n\nmodel.bin:  11%|#         | 304M/2.86G [00:06&lt;00:51, 49.9MB/s]\u001b[A\n\nmodel.bin:  11%|#1        | 315M/2.86G [00:06&lt;00:49, 51.5MB/s]\u001b[A\n\nmodel.bin:  11%|#1        | 325M/2.86G [00:06&lt;00:44, 56.5MB/s]\u001b[A\n\nmodel.bin:  12%|#1        | 336M/2.86G [00:06&lt;00:44, 57.1MB/s]\u001b[A\n\nmodel.bin:  12%|#2        | 346M/2.86G [00:07&lt;00:49, 51.1MB/s]\u001b[A\n\nmodel.bin:  12%|#2        | 357M/2.86G [00:07&lt;00:47, 53.0MB/s]\u001b[A\n\nmodel.bin:  13%|#2        | 367M/2.86G [00:07&lt;00:46, 53.0MB/s]\u001b[A\n\nmodel.bin:  13%|#3        | 377M/2.86G [00:07&lt;00:46, 53.8MB/s]\u001b[A\n\nmodel.bin:  14%|#3        | 388M/2.86G [00:07&lt;00:45, 54.1MB/s]\u001b[A\n\nmodel.bin:  14%|#3        | 398M/2.86G [00:07&lt;00:42, 58.4MB/s]\u001b[A\n\nmodel.bin:  14%|#4        | 409M/2.86G [00:08&lt;00:41, 58.3MB/s]\u001b[A\n\nmodel.bin:  15%|#4        | 419M/2.86G [00:08&lt;00:42, 58.0MB/s]\u001b[A\n\nmodel.bin:  15%|#5        | 430M/2.86G [00:08&lt;00:41, 58.0MB/s]\u001b[A\n\nmodel.bin:  15%|#5        | 440M/2.86G [00:08&lt;00:38, 62.2MB/s]\u001b[A\n\nmodel.bin:  16%|#5        | 451M/2.86G [00:08&lt;00:39, 60.6MB/s]\u001b[A\n\nmodel.bin:  16%|#6        | 461M/2.86G [00:08&lt;00:38, 61.6MB/s]\u001b[A\n\nmodel.bin:  17%|#6        | 472M/2.86G [00:09&lt;00:37, 63.4MB/s]\u001b[A\n\nmodel.bin:  17%|#6        | 482M/2.86G [00:09&lt;00:41, 57.1MB/s]\u001b[A\n\nmodel.bin:  17%|#7        | 493M/2.86G [00:09&lt;00:41, 56.7MB/s]\u001b[A\n\nmodel.bin:  18%|#7        | 503M/2.86G [00:09&lt;00:38, 61.9MB/s]\u001b[A\n\nmodel.bin:  18%|#7        | 514M/2.86G [00:09&lt;00:39, 60.0MB/s]\u001b[A\n\nmodel.bin:  18%|#8        | 524M/2.86G [00:09&lt;00:36, 64.5MB/s]\u001b[A\n\nmodel.bin:  19%|#8        | 535M/2.86G [00:10&lt;00:37, 61.9MB/s]\u001b[A\n\nmodel.bin:  19%|#9        | 545M/2.86G [00:10&lt;00:38, 60.0MB/s]\u001b[A\n\nmodel.bin:  19%|#9        | 556M/2.86G [00:10&lt;00:35, 64.6MB/s]\u001b[A\n\nmodel.bin:  20%|#9        | 566M/2.86G [00:10&lt;00:38, 60.0MB/s]\u001b[A\n\nmodel.bin:  20%|##        | 577M/2.86G [00:10&lt;00:43, 52.2MB/s]\u001b[A\n\nmodel.bin:  21%|##        | 587M/2.86G [00:11&lt;00:41, 55.2MB/s]\u001b[A\n\nmodel.bin:  21%|##        | 598M/2.86G [00:11&lt;00:38, 58.2MB/s]\u001b[A\n\nmodel.bin:  21%|##1       | 608M/2.86G [00:11&lt;00:46, 47.8MB/s]\u001b[A\n\nmodel.bin:  22%|##1       | 619M/2.86G [00:11&lt;00:42, 52.4MB/s]\u001b[A\n\nmodel.bin:  22%|##2       | 629M/2.86G [00:11&lt;00:40, 54.7MB/s]\u001b[A\n\nmodel.bin:  22%|##2       | 640M/2.86G [00:12&lt;00:37, 59.7MB/s]\u001b[A\n\nmodel.bin:  23%|##2       | 650M/2.86G [00:12&lt;00:37, 58.4MB/s]\u001b[A\n\nmodel.bin:  23%|##3       | 661M/2.86G [00:12&lt;00:38, 57.7MB/s]\u001b[A\n\nmodel.bin:  24%|##3       | 671M/2.86G [00:12&lt;00:36, 60.2MB/s]\u001b[A\n\nmodel.bin:  24%|##3       | 682M/2.86G [00:12&lt;00:35, 61.1MB/s]\u001b[A\n\nmodel.bin:  24%|##4       | 692M/2.86G [00:12&lt;00:36, 59.5MB/s]\u001b[A\n\nmodel.bin:  25%|##4       | 703M/2.86G [00:13&lt;00:33, 63.4MB/s]\u001b[A\n\nmodel.bin:  25%|##4       | 713M/2.86G [00:13&lt;00:35, 60.9MB/s]\u001b[A\n\nmodel.bin:  25%|##5       | 724M/2.86G [00:13&lt;00:36, 59.1MB/s]\u001b[A\n\nmodel.bin:  26%|##5       | 734M/2.86G [00:13&lt;00:33, 63.6MB/s]\u001b[A\n\nmodel.bin:  26%|##6       | 744M/2.86G [00:13&lt;00:41, 51.4MB/s]\u001b[A\n\nmodel.bin:  27%|##6       | 765M/2.86G [00:14&lt;00:31, 65.4MB/s]\u001b[A\n\nmodel.bin:  27%|##7       | 776M/2.86G [00:14&lt;00:33, 61.3MB/s]\u001b[A\n\nmodel.bin:  28%|##7       | 786M/2.86G [00:14&lt;00:32, 64.5MB/s]\u001b[A\n\nmodel.bin:  28%|##7       | 797M/2.86G [00:14&lt;00:33, 62.1MB/s]\u001b[A\n\nmodel.bin:  28%|##8       | 807M/2.86G [00:14&lt;00:33, 60.4MB/s]\u001b[A\n\nmodel.bin:  29%|##8       | 818M/2.86G [00:14&lt;00:31, 64.6MB/s]\u001b[A\n\nmodel.bin:  29%|##9       | 828M/2.86G [00:15&lt;00:32, 61.8MB/s]\u001b[A\n\nmodel.bin:  29%|##9       | 839M/2.86G [00:15&lt;00:30, 65.3MB/s]\u001b[A\n\nmodel.bin:  30%|##9       | 849M/2.86G [00:15&lt;00:32, 62.2MB/s]\u001b[A\n\nmodel.bin:  30%|###       | 860M/2.86G [00:15&lt;00:33, 60.3MB/s]\u001b[A\n\nmodel.bin:  30%|###       | 870M/2.86G [00:15&lt;00:30, 64.5MB/s]\u001b[A\n\nmodel.bin:  31%|###       | 881M/2.86G [00:15&lt;00:31, 61.8MB/s]\u001b[A\n\nmodel.bin:  31%|###1      | 891M/2.86G [00:16&lt;00:41, 47.0MB/s]\u001b[A\n\nmodel.bin:  32%|###1      | 912M/2.86G [00:16&lt;00:28, 68.8MB/s]\u001b[A\n\nmodel.bin:  32%|###2      | 923M/2.86G [00:16&lt;00:28, 68.0MB/s]\u001b[A\n\nmodel.bin:  33%|###2      | 933M/2.86G [00:16&lt;00:29, 64.3MB/s]\u001b[A\n\nmodel.bin:  33%|###3      | 944M/2.86G [00:16&lt;00:28, 67.9MB/s]\u001b[A\n\nmodel.bin:  33%|###3      | 954M/2.86G [00:17&lt;00:29, 63.9MB/s]\u001b[A\n\nmodel.bin:  34%|###3      | 965M/2.86G [00:17&lt;00:30, 61.0MB/s]\u001b[A\n\nmodel.bin:  34%|###4      | 975M/2.86G [00:17&lt;00:31, 59.1MB/s]\u001b[A\n\nmodel.bin:  35%|###4      | 986M/2.86G [00:17&lt;00:29, 63.5MB/s]\u001b[A\n\nmodel.bin:  35%|###4      | 996M/2.86G [00:17&lt;00:36, 50.7MB/s]\u001b[A\n\nmodel.bin:  35%|###5      | 1.01G/2.86G [00:18&lt;00:32, 56.4MB/s]\u001b[A\n\nmodel.bin:  36%|###5      | 1.02G/2.86G [00:18&lt;00:36, 50.3MB/s]\u001b[A\n\nmodel.bin:  36%|###5      | 1.03G/2.86G [00:18&lt;00:41, 44.3MB/s]\u001b[A\n\nmodel.bin:  36%|###6      | 1.04G/2.86G [00:18&lt;00:38, 47.4MB/s]\u001b[A\n\nmodel.bin:  37%|###6      | 1.05G/2.86G [00:19&lt;00:40, 44.1MB/s]\u001b[A\n\nmodel.bin:  37%|###7      | 1.06G/2.86G [00:19&lt;00:36, 49.9MB/s]\u001b[A\n\nmodel.bin:  37%|###7      | 1.07G/2.86G [00:19&lt;00:34, 51.8MB/s]\u001b[A\n\nmodel.bin:  38%|###7      | 1.08G/2.86G [00:19&lt;00:32, 54.1MB/s]\u001b[A\n\nmodel.bin:  38%|###8      | 1.09G/2.86G [00:19&lt;00:32, 53.8MB/s]\u001b[A\n\nmodel.bin:  39%|###8      | 1.10G/2.86G [00:19&lt;00:31, 55.0MB/s]\u001b[A\n\nmodel.bin:  39%|###8      | 1.11G/2.86G [00:20&lt;00:32, 54.4MB/s]\u001b[A\n\nmodel.bin:  39%|###9      | 1.12G/2.86G [00:20&lt;00:29, 58.2MB/s]\u001b[A\n\nmodel.bin:  40%|###9      | 1.13G/2.86G [00:20&lt;00:29, 57.8MB/s]\u001b[A\n\nmodel.bin:  40%|####      | 1.14G/2.86G [00:20&lt;00:31, 55.2MB/s]\u001b[A\n\nmodel.bin:  40%|####      | 1.15G/2.86G [00:20&lt;00:28, 59.0MB/s]\u001b[A\n\nmodel.bin:  41%|####      | 1.16G/2.86G [00:21&lt;00:28, 59.1MB/s]\u001b[A\n\nmodel.bin:  41%|####1     | 1.17G/2.86G [00:21&lt;00:26, 62.5MB/s]\u001b[A\n\nmodel.bin:  41%|####1     | 1.18G/2.86G [00:21&lt;00:29, 57.0MB/s]\u001b[A\n\nmodel.bin:  42%|####1     | 1.20G/2.86G [00:21&lt;00:29, 56.6MB/s]\u001b[A\n\nmodel.bin:  42%|####2     | 1.21G/2.86G [00:21&lt;00:28, 58.7MB/s]\u001b[A\n\nmodel.bin:  43%|####2     | 1.22G/2.86G [00:21&lt;00:29, 55.5MB/s]\u001b[A\n\nmodel.bin:  43%|####2     | 1.23G/2.86G [00:22&lt;00:27, 58.8MB/s]\u001b[A\n\nmodel.bin:  43%|####3     | 1.24G/2.86G [00:22&lt;00:28, 56.1MB/s]\u001b[A\n\nmodel.bin:  44%|####3     | 1.25G/2.86G [00:22&lt;00:28, 56.4MB/s]\u001b[A\n\nmodel.bin:  44%|####4     | 1.26G/2.86G [00:22&lt;00:27, 57.2MB/s]\u001b[A\n\nmodel.bin:  44%|####4     | 1.27G/2.86G [00:22&lt;00:27, 57.1MB/s]\u001b[A\n\nmodel.bin:  45%|####4     | 1.28G/2.86G [00:23&lt;00:26, 58.6MB/s]\u001b[A\n\nmodel.bin:  45%|####5     | 1.29G/2.86G [00:23&lt;00:25, 60.4MB/s]\u001b[A\n\nmodel.bin:  46%|####5     | 1.30G/2.86G [00:23&lt;00:25, 61.9MB/s]\u001b[A\n\nmodel.bin:  46%|####5     | 1.31G/2.86G [00:23&lt;00:24, 61.9MB/s]\u001b[A\n\nmodel.bin:  46%|####6     | 1.32G/2.86G [00:23&lt;00:26, 58.9MB/s]\u001b[A\n\nmodel.bin:  47%|####6     | 1.33G/2.86G [00:23&lt;00:24, 62.1MB/s]\u001b[A\n\nmodel.bin:  47%|####7     | 1.34G/2.86G [00:24&lt;00:25, 60.5MB/s]\u001b[A\n\nmodel.bin:  47%|####7     | 1.35G/2.86G [00:24&lt;00:25, 59.0MB/s]\u001b[A\n\nmodel.bin:  48%|####7     | 1.36G/2.86G [00:24&lt;00:24, 62.0MB/s]\u001b[A\n\nmodel.bin:  48%|####8     | 1.37G/2.86G [00:24&lt;00:24, 61.3MB/s]\u001b[A\n\nmodel.bin:  48%|####8     | 1.38G/2.86G [00:24&lt;00:23, 63.8MB/s]\u001b[A\n\nmodel.bin:  49%|####8     | 1.39G/2.86G [00:24&lt;00:23, 62.8MB/s]\u001b[A\n\nmodel.bin:  49%|####9     | 1.41G/2.86G [00:25&lt;00:22, 64.3MB/s]\u001b[A\n\nmodel.bin:  50%|####9     | 1.42G/2.86G [00:25&lt;00:26, 54.8MB/s]\u001b[A\n\nmodel.bin:  50%|####9     | 1.43G/2.86G [00:25&lt;00:24, 58.9MB/s]\u001b[A\n\nmodel.bin:  50%|#####     | 1.44G/2.86G [00:25&lt;00:24, 57.7MB/s]\u001b[A\n\nmodel.bin:  51%|#####     | 1.45G/2.86G [00:25&lt;00:22, 62.3MB/s]\u001b[A\n\nmodel.bin:  51%|#####1    | 1.46G/2.86G [00:25&lt;00:23, 59.7MB/s]\u001b[A\n\nmodel.bin:  51%|#####1    | 1.47G/2.86G [00:26&lt;00:24, 57.6MB/s]\u001b[A\n\nmodel.bin:  52%|#####1    | 1.48G/2.86G [00:26&lt;00:22, 60.4MB/s]\u001b[A\n\nmodel.bin:  52%|#####2    | 1.49G/2.86G [00:26&lt;00:22, 60.3MB/s]\u001b[A\n\nmodel.bin:  53%|#####2    | 1.50G/2.86G [00:26&lt;00:22, 60.9MB/s]\u001b[A\n\nmodel.bin:  53%|#####2    | 1.51G/2.86G [00:26&lt;00:26, 50.4MB/s]\u001b[A\n\nmodel.bin:  53%|#####3    | 1.52G/2.86G [00:27&lt;00:25, 51.9MB/s]\u001b[A\n\nmodel.bin:  54%|#####3    | 1.53G/2.86G [00:27&lt;00:22, 57.6MB/s]\u001b[A\n\nmodel.bin:  54%|#####3    | 1.54G/2.86G [00:27&lt;00:22, 57.4MB/s]\u001b[A\n\nmodel.bin:  54%|#####4    | 1.55G/2.86G [00:27&lt;00:23, 55.1MB/s]\u001b[A\n\nmodel.bin:  55%|#####4    | 1.56G/2.86G [00:29&lt;01:09, 18.5MB/s]\u001b[A\n\nmodel.bin:  55%|#####5    | 1.57G/2.86G [00:29&lt;00:53, 23.9MB/s]\u001b[A\n\nmodel.bin:  56%|#####5    | 1.59G/2.86G [00:29&lt;00:31, 40.3MB/s]\u001b[A\n\nmodel.bin:  56%|#####6    | 1.60G/2.86G [00:30&lt;01:01, 20.4MB/s]\u001b[A\n\nmodel.bin:  57%|#####6    | 1.63G/2.86G [00:30&lt;00:38, 31.6MB/s]\u001b[A\n\nmodel.bin:  58%|#####7    | 1.65G/2.86G [00:32&lt;00:54, 22.2MB/s]\u001b[A\n\nmodel.bin:  58%|#####8    | 1.67G/2.86G [00:32&lt;00:38, 31.2MB/s]\u001b[A\n\nmodel.bin:  59%|#####9    | 1.70G/2.86G [00:32&lt;00:23, 49.6MB/s]\u001b[A\n\nmodel.bin:  61%|######    | 1.73G/2.86G [00:32&lt;00:16, 70.3MB/s]\u001b[A\n\nmodel.bin:  61%|######1   | 1.75G/2.86G [00:32&lt;00:16, 68.7MB/s]\u001b[A\n\nmodel.bin:  62%|######2   | 1.77G/2.86G [00:33&lt;00:16, 64.3MB/s]\u001b[A\n\nmodel.bin:  63%|######2   | 1.79G/2.86G [00:33&lt;00:17, 60.7MB/s]\u001b[A\n\nmodel.bin:  63%|######3   | 1.80G/2.86G [00:34&lt;00:18, 57.1MB/s]\u001b[A\n\nmodel.bin:  64%|######3   | 1.81G/2.86G [00:34&lt;00:18, 57.3MB/s]\u001b[A\n\nmodel.bin:  64%|######3   | 1.82G/2.86G [00:34&lt;00:18, 57.1MB/s]\u001b[A\n\nmodel.bin:  64%|######4   | 1.84G/2.86G [00:34&lt;00:16, 60.2MB/s]\u001b[A\n\nmodel.bin:  65%|######4   | 1.85G/2.86G [00:34&lt;00:16, 59.6MB/s]\u001b[A\n\nmodel.bin:  65%|######4   | 1.86G/2.86G [00:34&lt;00:15, 62.8MB/s]\u001b[A\n\nmodel.bin:  65%|######5   | 1.87G/2.86G [00:35&lt;00:16, 61.4MB/s]\u001b[A\n\nmodel.bin:  66%|######5   | 1.88G/2.86G [00:35&lt;00:15, 64.1MB/s]\u001b[A\n\nmodel.bin:  66%|######6   | 1.89G/2.86G [00:35&lt;00:15, 63.1MB/s]\u001b[A\n\nmodel.bin:  66%|######6   | 1.90G/2.86G [00:35&lt;00:15, 61.1MB/s]\u001b[A\n\nmodel.bin:  67%|######6   | 1.91G/2.86G [00:35&lt;00:14, 63.4MB/s]\u001b[A\n\nmodel.bin:  67%|######7   | 1.92G/2.86G [00:35&lt;00:14, 62.5MB/s]\u001b[A\n\nmodel.bin:  68%|######7   | 1.93G/2.86G [00:35&lt;00:14, 64.9MB/s]\u001b[A\n\nmodel.bin:  68%|######7   | 1.94G/2.86G [00:36&lt;00:14, 63.7MB/s]\u001b[A\n\nmodel.bin:  68%|######8   | 1.95G/2.86G [00:36&lt;00:14, 61.1MB/s]\u001b[A\n\nmodel.bin:  69%|######8   | 1.96G/2.86G [00:36&lt;00:14, 63.7MB/s]\u001b[A\n\nmodel.bin:  69%|######9   | 1.97G/2.86G [00:36&lt;00:14, 62.5MB/s]\u001b[A\n\nmodel.bin:  69%|######9   | 1.98G/2.86G [00:36&lt;00:13, 65.5MB/s]\u001b[A\n\nmodel.bin:  70%|######9   | 1.99G/2.86G [00:36&lt;00:13, 63.1MB/s]\u001b[A\n\nmodel.bin:  70%|#######   | 2.00G/2.86G [00:37&lt;00:17, 47.8MB/s]\u001b[A\n\nmodel.bin:  71%|#######   | 2.02G/2.86G [00:37&lt;00:12, 66.7MB/s]\u001b[A\n\nmodel.bin:  71%|#######1  | 2.03G/2.86G [00:37&lt;00:12, 64.9MB/s]\u001b[A\n\nmodel.bin:  72%|#######1  | 2.04G/2.86G [00:37&lt;00:12, 62.4MB/s]\u001b[A\n\nmodel.bin:  72%|#######1  | 2.06G/2.86G [00:38&lt;00:17, 46.7MB/s]\u001b[A\n\nmodel.bin:  72%|#######2  | 2.07G/2.86G [00:38&lt;00:16, 49.1MB/s]\u001b[A\n\nmodel.bin:  73%|#######2  | 2.08G/2.86G [00:38&lt;00:15, 50.8MB/s]\u001b[A\n\nmodel.bin:  73%|#######3  | 2.09G/2.86G [00:38&lt;00:13, 56.4MB/s]\u001b[A\n\nmodel.bin:  73%|#######3  | 2.10G/2.86G [00:38&lt;00:13, 55.9MB/s]\u001b[A\n\nmodel.bin:  74%|#######3  | 2.11G/2.86G [00:39&lt;00:13, 55.8MB/s]\u001b[A\n\nmodel.bin:  74%|#######4  | 2.12G/2.86G [00:39&lt;00:12, 60.7MB/s]\u001b[A\n\nmodel.bin:  75%|#######4  | 2.13G/2.86G [00:39&lt;00:12, 58.9MB/s]\u001b[A\n\nmodel.bin:  75%|#######4  | 2.14G/2.86G [00:39&lt;00:11, 63.5MB/s]\u001b[A\n\nmodel.bin:  75%|#######5  | 2.15G/2.86G [00:39&lt;00:11, 61.1MB/s]\u001b[A\n\nmodel.bin:  76%|#######5  | 2.16G/2.86G [00:39&lt;00:10, 65.0MB/s]\u001b[A\n\nmodel.bin:  76%|#######6  | 2.17G/2.86G [00:40&lt;00:11, 61.6MB/s]\u001b[A\n\nmodel.bin:  76%|#######6  | 2.18G/2.86G [00:40&lt;00:11, 59.6MB/s]\u001b[A\n\nmodel.bin:  77%|#######6  | 2.19G/2.86G [00:40&lt;00:10, 63.7MB/s]\u001b[A\n\nmodel.bin:  77%|#######7  | 2.20G/2.86G [00:40&lt;00:10, 61.4MB/s]\u001b[A\n\nmodel.bin:  77%|#######7  | 2.21G/2.86G [00:40&lt;00:09, 64.9MB/s]\u001b[A\n\nmodel.bin:  78%|#######7  | 2.22G/2.86G [00:40&lt;00:10, 62.0MB/s]\u001b[A\n\nmodel.bin:  78%|#######8  | 2.23G/2.86G [00:41&lt;00:09, 66.2MB/s]\u001b[A\n\nmodel.bin:  79%|#######8  | 2.24G/2.86G [00:41&lt;00:09, 62.7MB/s]\u001b[A\n\nmodel.bin:  79%|#######8  | 2.25G/2.86G [00:41&lt;00:09, 60.7MB/s]\u001b[A\n\nmodel.bin:  79%|#######9  | 2.26G/2.86G [00:41&lt;00:09, 64.6MB/s]\u001b[A\n\nmodel.bin:  80%|#######9  | 2.28G/2.86G [00:41&lt;00:11, 48.7MB/s]\u001b[A\n\nmodel.bin:  80%|########  | 2.29G/2.86G [00:42&lt;00:11, 50.8MB/s]\u001b[A\n\nmodel.bin:  80%|########  | 2.30G/2.86G [00:42&lt;00:10, 52.1MB/s]\u001b[A\n\nmodel.bin:  81%|########  | 2.31G/2.86G [00:42&lt;00:09, 57.7MB/s]\u001b[A\n\nmodel.bin:  81%|########1 | 2.32G/2.86G [00:42&lt;00:09, 57.1MB/s]\u001b[A\n\nmodel.bin:  82%|########1 | 2.33G/2.86G [00:42&lt;00:08, 62.1MB/s]\u001b[A\n\nmodel.bin:  82%|########1 | 2.34G/2.86G [00:42&lt;00:08, 60.2MB/s]\u001b[A\n\nmodel.bin:  82%|########2 | 2.35G/2.86G [00:43&lt;00:08, 58.4MB/s]\u001b[A\n\nmodel.bin:  83%|########2 | 2.36G/2.86G [00:43&lt;00:08, 57.5MB/s]\u001b[A\n\nmodel.bin:  83%|########2 | 2.37G/2.86G [00:43&lt;00:07, 62.5MB/s]\u001b[A\n\nmodel.bin:  83%|########3 | 2.38G/2.86G [00:43&lt;00:07, 60.0MB/s]\u001b[A\n\nmodel.bin:  84%|########3 | 2.39G/2.86G [00:43&lt;00:07, 59.2MB/s]\u001b[A\n\nmodel.bin:  84%|########4 | 2.40G/2.86G [00:43&lt;00:07, 63.4MB/s]\u001b[A\n\nmodel.bin:  84%|########4 | 2.41G/2.86G [00:44&lt;00:07, 60.5MB/s]\u001b[A\n\nmodel.bin:  85%|########4 | 2.42G/2.86G [00:44&lt;00:06, 65.2MB/s]\u001b[A\n\nmodel.bin:  85%|########5 | 2.43G/2.86G [00:44&lt;00:06, 62.1MB/s]\u001b[A\n\nmodel.bin:  86%|########5 | 2.44G/2.86G [00:44&lt;00:06, 59.8MB/s]\u001b[A\n\nmodel.bin:  86%|########5 | 2.45G/2.86G [00:44&lt;00:06, 64.1MB/s]\u001b[A\n\nmodel.bin:  86%|########6 | 2.46G/2.86G [00:45&lt;00:08, 44.5MB/s]\u001b[A\n\nmodel.bin:  87%|########6 | 2.47G/2.86G [00:45&lt;00:07, 50.7MB/s]\u001b[A\n\nmodel.bin:  87%|########7 | 2.49G/2.86G [00:45&lt;00:08, 43.1MB/s]\u001b[A\n\nmodel.bin:  87%|########7 | 2.50G/2.86G [00:45&lt;00:07, 47.1MB/s]\u001b[A\n\nmodel.bin:  88%|########7 | 2.51G/2.86G [00:46&lt;00:07, 48.2MB/s]\u001b[A\n\nmodel.bin:  88%|########8 | 2.52G/2.86G [00:46&lt;00:06, 54.3MB/s]\u001b[A\n\nmodel.bin:  88%|########8 | 2.53G/2.86G [00:46&lt;00:05, 55.0MB/s]\u001b[A\n\nmodel.bin:  89%|########8 | 2.54G/2.86G [00:46&lt;00:05, 54.9MB/s]\u001b[A\n\nmodel.bin:  89%|########9 | 2.55G/2.86G [00:46&lt;00:05, 56.3MB/s]\u001b[A\n\nmodel.bin:  90%|########9 | 2.56G/2.86G [00:46&lt;00:04, 59.7MB/s]\u001b[A\n\nmodel.bin:  90%|########9 | 2.57G/2.86G [00:47&lt;00:04, 58.1MB/s]\u001b[A\n\nmodel.bin:  90%|######### | 2.58G/2.86G [00:47&lt;00:04, 62.2MB/s]\u001b[A\n\nmodel.bin:  91%|######### | 2.59G/2.86G [00:47&lt;00:04, 59.7MB/s]\u001b[A\n\nmodel.bin:  91%|#########1| 2.60G/2.86G [00:47&lt;00:04, 58.4MB/s]\u001b[A\n\nmodel.bin:  91%|#########1| 2.61G/2.86G [00:47&lt;00:04, 58.5MB/s]\u001b[A\n\nmodel.bin:  92%|#########1| 2.62G/2.86G [00:47&lt;00:03, 62.0MB/s]\u001b[A\n\nmodel.bin:  92%|#########2| 2.63G/2.86G [00:48&lt;00:03, 59.8MB/s]\u001b[A\n\nmodel.bin:  93%|#########2| 2.64G/2.86G [00:48&lt;00:03, 63.8MB/s]\u001b[A\n\nmodel.bin:  93%|#########2| 2.65G/2.86G [00:48&lt;00:03, 61.6MB/s]\u001b[A\n\nmodel.bin:  93%|#########3| 2.66G/2.86G [00:49&lt;00:09, 20.6MB/s]\u001b[A\n\nmodel.bin:  94%|#########3| 2.67G/2.86G [00:49&lt;00:07, 25.7MB/s]\u001b[A\n\nmodel.bin:  94%|#########4| 2.69G/2.86G [00:50&lt;00:03, 42.5MB/s]\u001b[A\n\nmodel.bin:  95%|#########5| 2.72G/2.86G [00:51&lt;00:05, 26.1MB/s]\u001b[A\n\nmodel.bin:  96%|#########5| 2.74G/2.86G [00:51&lt;00:03, 37.1MB/s]\u001b[A\n\nmodel.bin:  97%|#########6| 2.76G/2.86G [00:51&lt;00:01, 51.5MB/s]\u001b[A\n\nmodel.bin:  98%|#########7| 2.79G/2.86G [00:51&lt;00:00, 77.4MB/s]\u001b[A\n\nmodel.bin:  99%|#########8| 2.82G/2.86G [00:51&lt;00:00, 91.5MB/s]\u001b[A\n\nmodel.bin: 100%|#########9| 2.84G/2.86G [00:52&lt;00:00, 74.5MB/s]\u001b[A\n\nmodel.bin: 100%|##########| 2.86G/2.86G [00:53&lt;00:00, 48.1MB/s]\u001b[A\nmodel.bin: 100%|##########| 2.86G/2.86G [00:53&lt;00:00, 53.8MB/s]\n\nFetching 7 files:  43%|####2     | 3/7 [00:54&lt;01:38, 24.68s/it]\n\nshared_vocabulary.txt:   0%|          | 0.00/299k [00:00&lt;?, ?B/s]\u001b[A\nshared_vocabulary.txt: 100%|##########| 299k/299k [00:00&lt;00:00, 4.05MB/s]\n\nFetching 7 files:  57%|#####7    | 4/7 [00:54&lt;00:45, 15.04s/it]\n\nspecial_tokens_map.json:   0%|          | 0.00/2.20k [00:00&lt;?, ?B/s]\u001b[A\nspecial_tokens_map.json: 100%|##########| 2.20k/2.20k [00:00&lt;00:00, 12.7MB/s]\n\nFetching 7 files:  71%|#######1  | 5/7 [00:54&lt;00:19,  9.73s/it]\n\ntokenizer_config.json:   0%|          | 0.00/2.35k [00:00&lt;?, ?B/s]\u001b[A\ntokenizer_config.json: 100%|##########| 2.35k/2.35k [00:00&lt;00:00, 14.4MB/s]\n\nFetching 7 files: 100%|##########| 7/7 [00:55&lt;00:00,  4.84s/it]\nFetching 7 files: 100%|##########| 7/7 [00:55&lt;00:00,  7.86s/it]\n\n\nYeah, here we go little (4gb) buddy!\n\nNow that we got the basics, let’s play with it! You can change the prompt like such\n\nlm.chat('''\n     System: Respond as a physics professor.\n\n     User: What is relativity?\n\n     Assistant:\n     ''')\n\n'Relativity is a branch of physics that deals with the behavior and interactions between objects in space-time. It was proposed by Albert Einstein, who introduced it as a theory to explain how light travels through space-time at different speeds depending on the distance from which it is observed. The theory has been widely accepted throughout history and has revolutionized our understanding of the universe.'\n\n\n\n\nlm.complete(\"She hid in her room until\")\n\n' the power went out and she could call for help.'\n\n\n\n\nlm.get_wiki(\"Physics\")\n\n'Physics is the natural science of matter, involving the study of matter, its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force. Physics is one of the most fundamental scientific disciplines, with its main goal being to understand how the universe behaves. A scientist who specializes in the field of physics is called a physicist.\\n\\nPhysics is one of the oldest academic disciplines and, through its inclusion of astronomy, perhaps the oldest. Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century these natural sciences emerged as unique research endeavors in their own right. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in these and other academic disciplines such as mathematics and philosophy.\\n\\nAdvances in physics often enable new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.'"
  },
  {
    "objectID": "blog/Data_Viz_Fundamentals/index.html",
    "href": "blog/Data_Viz_Fundamentals/index.html",
    "title": "Data Viz Fundamentals with R",
    "section": "",
    "text": "By David Munoz Tord\nGoal: Go over the basics of data visualization with R and learn more advanced concepts using the ggplot package.\nNote: All is in english\nOpen it full"
  },
  {
    "objectID": "blog/R-Lunches/index.html",
    "href": "blog/R-Lunches/index.html",
    "title": "R Lunches in university of Geneva",
    "section": "",
    "text": "By David Munoz Tord\nR lunches are multidisciplinary meetings on R at UniMail.\nWe finished this semester R Lunches but you can still find the video links if you missed one!\nRead more about it"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nDbVieweR\n\n\n\n\n\n\n\nProject\n\n\nPackage\n\n\nR\n\n\nDBMS\n\n\nSQL\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n  \n\n\n\n\nEcharts4r\n\n\n\n\n\n\n\nProject\n\n\nPackage\n\n\nR\n\n\nInteractive\n\n\nDataViz\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2022\n\n\nDavid Munoz Tord\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "services/data_engineering/index.html",
    "href": "services/data_engineering/index.html",
    "title": "Data Engineering",
    "section": "",
    "text": "A data project always start with a data engineering step. You need to harvest data and store it in an adequate format to be able to use it. Let me help you with it !\nI can help you to:\n\n Harvest data from various sources, including APIs, SQL databases, or complex Excel spreadsheets.\n Reformat and reshape data to facilitate further analysis.\n Clean datasets to remove outliers and aberrant data points.\n Develop reproducible data preparation workflows.\n Automate the creation of data-related reports.\n\n\n\n\n\n\n Led development and launch of a collaborative project with EPFL, transitioning scripts into a Software as a Service (SaaS) solution on AWS.\n Provided technical consulting to a startup, translating academic research into actionable solutions and advising on machine learning.\n Collaborated across departments to design and deploy data platform initiatives, implementing automation for time savings.\n\n\n\n\n\nIf you’re interested in learning more about how I can assist with your project, feel free to reach out at david.munoztord@mailbox.org."
  },
  {
    "objectID": "services/data_engineering/index.html#fa-cog-data-engineering",
    "href": "services/data_engineering/index.html#fa-cog-data-engineering",
    "title": "Data Engineering",
    "section": "",
    "text": "A data project always start with a data engineering step. You need to harvest data and store it in an adequate format to be able to use it. Let me help you with it !\nI can help you to:\n\n Harvest data from various sources, including APIs, SQL databases, or complex Excel spreadsheets.\n Reformat and reshape data to facilitate further analysis.\n Clean datasets to remove outliers and aberrant data points.\n Develop reproducible data preparation workflows.\n Automate the creation of data-related reports.\n\n\n\n\n\n\n Led development and launch of a collaborative project with EPFL, transitioning scripts into a Software as a Service (SaaS) solution on AWS.\n Provided technical consulting to a startup, translating academic research into actionable solutions and advising on machine learning.\n Collaborated across departments to design and deploy data platform initiatives, implementing automation for time savings.\n\n\n\n\n\nIf you’re interested in learning more about how I can assist with your project, feel free to reach out at david.munoztord@mailbox.org."
  },
  {
    "objectID": "index.html#fa-hand-peace-hi-im-david.-fa-hand-peace-im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project",
    "href": "index.html#fa-hand-peace-hi-im-david.-fa-hand-peace-im-a-data-scientist-r-developer-always-looking-for-a-cool-open-source-project",
    "title": "David Munoz Tord",
    "section": " Hi, I’m David.   I’m a Data Scientist / R Developer always looking for a cool open-source project ! ",
    "text": "Hi, I’m David.   I’m a Data Scientist / R Developer always looking for a cool open-source project !"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "tl;dr\n\nPassionated about code  Bayesian stats  DevOps  Neurosciences  machine learning  beer  wine  coffee  travel  and striving for balance  I love abstract and conceptual thinking and am always trying to learn something new"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Contact\n\n\nContact me at david.munoztord@mailbox.org."
  },
  {
    "objectID": "vitae.html",
    "href": "vitae.html",
    "title": "Full CV",
    "section": "",
    "text": "I believe that the 21st century requires a more multifaceted toolkit if we hope to solve complex challenges–process and task oriented approaches balanced within a framework that fosters strategic thinking. That interdisciplinary relationships can be strengthened through active community participation–working toward something with the shared value of doing good.\n I bring a lot of energy and enthusiasm to the projects that I become involved in. I thrive in situations that require imagination and innovation. My interests are varied; among my passions I would list: academic inquiry, classical and contemporary thought, the music and cuisine of other cultures, the arts, traveling, languages, and social justice. I believe that our most vulnerable deserve respect and to be treated with dignity and fairness.\n Ultimately I hope that the work I do is helpful–that it enables discovery or provides clarity. I hope to bring together practical and principled approaches and I hope to further grow as this philosophy guides me.\n\n\n\n\n\n\n I am dedicated and hard working — a good communicator and active community participant. I work well on team efforts and projects with the ultimate aim of using data for social good.\n\n\n\n I am:\n\n creative\n honest\n friendly\n enthusiastic\n keen to learn new things and take on new roles\n reliable\n a lateral thinker\n motivated\n\n\n\n\n I am able to:\n\n listen to and follow instructions accurately\n work cooperatively\n take on new challenges\n complete tasks\n support others\n identify new innovations and technologies while working toward implementing them\n be proactive\n speak and read in French, Spanish, and English\n read and understand some Japanese\n\n\n\n\n\n\n\n Coding: R, Python, STAN, BASH, SQL and JavaScript.\n Data: Relational Databases (data modeling and SQL); Extract-Transfer-Load (ETL) strategies for data engineering — specifically with Apache NIFI for ETL; information retrieval; and, knowledge organizational theory. I have created my own DBMS in R and Shiny that uses PostgreSQL and SQLite as back-ends.\n Data Science: both R and Python are a big part of my workflow from reading in data, transforming and tidying data, exploring and visualizing data, modeling, reporting, presenting, and tying it all together in a dashboard. In connection to R, I am a proponent of the tidyverse framework for its unifying API. With respect to Python, I enjoy using polars, pyjanitor, pymc, and plotly to work in equivalent ways. However I honestly prefer working in R. I am of course familiar with the scikit-learn and tensorflow libraries in Python. I have experience with spark as well, plus torch and keras for R. I have also worked with brms and hBayesDM for Bayesian modeling.\n Web Technologies: HTML, CSS, Javascript; Quarto publishing; Rmarkdown Websites, Dashboards in R with shiny/flexdashboard that support crosstalk, and interactive data visualization with echarts; Static Site Generators, i.e., Hugo, and others.\n OSs: macOS, Linux, and containers. Within the Linux ecosystem, I have experience with Debian based distributions as well as Red Hat Enterprise Linux. I love the terminal and wish vim bindings were universal.\n DevOps tools: Fair knowledge of AWS and its services, including S3, Sagemaker, Glue, and EKS. I have also worked with Azure DevOps. I am very familiar with git and github and have experience with github actions for CI/CD. I have also worked with Docker and I have some MlOps experience.\n\n\n\n\n\n Doctor of Philosophy (Ph.D.) in Neurosciences - University of Geneva, Switerland / Mar 2019 ‑ Nov 2022\n\nResearch focus on computational models of addiction\nThesis Title: “Learning and Decision Making in Addiction: A Computational Approach”\nDiscontinued studies after 2.5 years\n\n Master in Neurosciences - University of Geneva, Switzerland / Sep 2018 ‑ Mar 2019 \n\nThesis: “Differential contributions of ventral striatum subregions to the affective processing of reward”\nCollaborated with CalTech data engineers to implement tICA for better identification of activation and artifact components in fMRI.\n\n Complementary Studies in Data Science - Smith College, MA, USA / Sep 2017 ‑ Jun 2018\n\nCapstone project: Leveraged SQL and R to create a comprehensive analysis of the entertainment industry’s network using the IMDb database.\n\n Bachelor of Science in Psychology - University of Geneva, Switzerland /Sep 2014 ‑ Jun 2017\n\nRelevant coursework completed in statistics and scientific programming.\n\n\n\n\n\n\n Data Sctuctures and Algorithms\n Data Mining\n Machine Learning\n Artificial Intelligence\n Linux System Administration\n Visualization for Scientific Data\n Database Management Systems\n Object-Oriented Programming\n Spatial Analysis\n\n\n\n\n\n\n\n\n\n Led the development and launch of a scientific project in collaboration with EPFL, transitioning initial scripts into a comprehensive Software as a Service (SaaS) solution accessible to clients via Elastic Kubernetes Service (EKS) on AWS.\n\n\n Offered technical consulting services to a startup specializing in cognitive and educational technologies, translating academic research in cognitive neuroscience and developmental psychology into actionable solutions, while also advising on machine learning implementations.\n\n\n Collaborated closely with analysts across departments to design, build, and deploy various initiatives within the data platform, including the development, deployment, and maintenance of data services using R and PostgreSQL. Additionally, implemented best practices for continuous process automation, resulting in significant time savings for month-end processing.\n\n\n \n\n\n\n\n Conducted in-depth data analysis to classify customers using Bayesian inference, revealing distinct segments and patterns for data-driven decision-making and targeted strategies.\n\n\n Designed and implemented Apache NiFi ETL for efficient data processing and orchestration, contributing to streamlined data engineering workflows.\n\n\n Developed new internal tools for data analysts.\n\n\n \n\n\n\n\n Led the development of the data science infrastructure, creating an environment tailored for cutting-edge research and innovation.\n\n\n Developed and implemented complex statistical models in R to detect fraudulent transactions.\n\n\n Designed and maintained a data visualization dashboard in R Shiny, which allowed for easy monitoring of key performance indicators.\n\n\n \n\n\n\n\n Conducted lectures on statistics for the graduate program.\n\n\n Mentored and supervised graduate students in their statistical research projects, nurturing their analytical skills and research capabilities.\n\n\n Created and shared freely available data workshops in R and Python.\n\n\n\n\n\n\n\n Led an independent research project in Pr. Wraga’s visual cognition group.\n\n\n Designed and collected data for a cognitive neuroscience experiment.\n\n\n\n\n\n\n\n Led an independent research project as part of Dr. Burra’s attentional capture group.\n\n\n Conducted signal processing of EEG data.\n\n\n\n\n\n\n\n echarts4r\n\nR, JavaScript - 2023 - Present\nCollaborated on the development of echarts4r, a R package that expands the capabilities of R-based interactive visualizations with a powerful rendering engine, while enhancing flexibility and ease of use.\n\n\n\n\n Firebase\n\nR, JavaScript - 2023 - Present\nContributed to the firebase integration package for R, empowering the Shiny user community by enabling user authentication and secure file storage through Firebase Storage.\n\n\n\n\n DbVieweR\n\nR, SQL - 2022 - Present\nCreated a shiny app simulating a database management system featuring functions like login authentication, save/create/delete tables, add/rename columns, using either a PostgreSQL or SQLite back-end. DbVieweR\n\n\n\n\n brms\n\nR - 2022\nMade small contributions to the brms package, including resolving issues related to class definition.\n\n reprtree\n\nR - 2022\nIntegrated the caret ensemble for reprtree (implementation of representative trees from ensembles of tree-based machines).\n\n hBayesDM\n\nSTAN, R, Python - 2021\nCollaborated on the Hierarchical Bayesian modeling of Decision-Making tasks library. Built Q-learning algorithm for probabilistic selection task. hBayesDM\n\n 3dLMEr\n\nBASH, R - 2020\nCollaborated on the AFNI’s functions for 3Dimensional Linear Mixed-Effects Regression. Fixed residuals output image by adding bottom tolerance. 3dLMEr\n\n\n\n\n\n\n\n\n French (Mother Tongue)\n Spanish (Mother Tongue)\n English (Fluent)\n German (Basic)\n Italian (Basic)\n\n\n\n\n\n\n Top 2% on Stack Overflow in 2024\n Top 25% on GitHub in 2023\n Global Fellow scholarship recipient in 2017\n\n\n\n\n\n\n Contributing to We Data’s mission by facilitating workshops, providing statistics tutorials, and participating in coding demonstrations.\n Organizing R-Lunches at UniGe.\n Co-founder of Go-Fast, a socially responsible Bike Messenger cooperative in Geneva.\n Co-president of La Rustine, a non-profit organization promoting non-motorized mobility and self-sustainability in Switzerland.\n\n\n\n\n\n\n Artificial Intelligence\n Machine Learning\n Bayesian Statistics\n Natural Language Processing\n Data Visualization\n Ethics in Technology\n Data Privacy\n Data Engineering\n Cybersecurity\n Data Journalism\n\n  References Available upon request."
  },
  {
    "objectID": "vitae.html#fa-compass-personal-philosophy",
    "href": "vitae.html#fa-compass-personal-philosophy",
    "title": "Full CV",
    "section": "",
    "text": "I believe that the 21st century requires a more multifaceted toolkit if we hope to solve complex challenges–process and task oriented approaches balanced within a framework that fosters strategic thinking. That interdisciplinary relationships can be strengthened through active community participation–working toward something with the shared value of doing good.\n I bring a lot of energy and enthusiasm to the projects that I become involved in. I thrive in situations that require imagination and innovation. My interests are varied; among my passions I would list: academic inquiry, classical and contemporary thought, the music and cuisine of other cultures, the arts, traveling, languages, and social justice. I believe that our most vulnerable deserve respect and to be treated with dignity and fairness.\n Ultimately I hope that the work I do is helpful–that it enables discovery or provides clarity. I hope to bring together practical and principled approaches and I hope to further grow as this philosophy guides me."
  },
  {
    "objectID": "vitae.html#fa-magic-personal-skills",
    "href": "vitae.html#fa-magic-personal-skills",
    "title": "Full CV",
    "section": "",
    "text": "I am dedicated and hard working — a good communicator and active community participant. I work well on team efforts and projects with the ultimate aim of using data for social good.\n\n\n\n I am:\n\n creative\n honest\n friendly\n enthusiastic\n keen to learn new things and take on new roles\n reliable\n a lateral thinker\n motivated\n\n\n\n\n I am able to:\n\n listen to and follow instructions accurately\n work cooperatively\n take on new challenges\n complete tasks\n support others\n identify new innovations and technologies while working toward implementing them\n be proactive\n speak and read in French, Spanish, and English\n read and understand some Japanese"
  },
  {
    "objectID": "vitae.html#fa-code-technical-skills",
    "href": "vitae.html#fa-code-technical-skills",
    "title": "Full CV",
    "section": "",
    "text": "Coding: R, Python, STAN, BASH, SQL and JavaScript.\n Data: Relational Databases (data modeling and SQL); Extract-Transfer-Load (ETL) strategies for data engineering — specifically with Apache NIFI for ETL; information retrieval; and, knowledge organizational theory. I have created my own DBMS in R and Shiny that uses PostgreSQL and SQLite as back-ends.\n Data Science: both R and Python are a big part of my workflow from reading in data, transforming and tidying data, exploring and visualizing data, modeling, reporting, presenting, and tying it all together in a dashboard. In connection to R, I am a proponent of the tidyverse framework for its unifying API. With respect to Python, I enjoy using polars, pyjanitor, pymc, and plotly to work in equivalent ways. However I honestly prefer working in R. I am of course familiar with the scikit-learn and tensorflow libraries in Python. I have experience with spark as well, plus torch and keras for R. I have also worked with brms and hBayesDM for Bayesian modeling.\n Web Technologies: HTML, CSS, Javascript; Quarto publishing; Rmarkdown Websites, Dashboards in R with shiny/flexdashboard that support crosstalk, and interactive data visualization with echarts; Static Site Generators, i.e., Hugo, and others.\n OSs: macOS, Linux, and containers. Within the Linux ecosystem, I have experience with Debian based distributions as well as Red Hat Enterprise Linux. I love the terminal and wish vim bindings were universal.\n DevOps tools: Fair knowledge of AWS and its services, including S3, Sagemaker, Glue, and EKS. I have also worked with Azure DevOps. I am very familiar with git and github and have experience with github actions for CI/CD. I have also worked with Docker and I have some MlOps experience."
  },
  {
    "objectID": "vitae.html#fa-graduation-cap-education",
    "href": "vitae.html#fa-graduation-cap-education",
    "title": "Full CV",
    "section": "",
    "text": "Doctor of Philosophy (Ph.D.) in Neurosciences - University of Geneva, Switerland / Mar 2019 ‑ Nov 2022\n\nResearch focus on computational models of addiction\nThesis Title: “Learning and Decision Making in Addiction: A Computational Approach”\nDiscontinued studies after 2.5 years\n\n Master in Neurosciences - University of Geneva, Switzerland / Sep 2018 ‑ Mar 2019 \n\nThesis: “Differential contributions of ventral striatum subregions to the affective processing of reward”\nCollaborated with CalTech data engineers to implement tICA for better identification of activation and artifact components in fMRI.\n\n Complementary Studies in Data Science - Smith College, MA, USA / Sep 2017 ‑ Jun 2018\n\nCapstone project: Leveraged SQL and R to create a comprehensive analysis of the entertainment industry’s network using the IMDb database.\n\n Bachelor of Science in Psychology - University of Geneva, Switzerland /Sep 2014 ‑ Jun 2017\n\nRelevant coursework completed in statistics and scientific programming."
  },
  {
    "objectID": "vitae.html#fa-chart-bar-coursework",
    "href": "vitae.html#fa-chart-bar-coursework",
    "title": "Full CV",
    "section": "",
    "text": "Data Sctuctures and Algorithms\n Data Mining\n Machine Learning\n Artificial Intelligence\n Linux System Administration\n Visualization for Scientific Data\n Database Management Systems\n Object-Oriented Programming\n Spatial Analysis"
  },
  {
    "objectID": "vitae.html#fa-institution-work-experience",
    "href": "vitae.html#fa-institution-work-experience",
    "title": "Full CV",
    "section": "",
    "text": "Led the development and launch of a scientific project in collaboration with EPFL, transitioning initial scripts into a comprehensive Software as a Service (SaaS) solution accessible to clients via Elastic Kubernetes Service (EKS) on AWS.\n\n\n Offered technical consulting services to a startup specializing in cognitive and educational technologies, translating academic research in cognitive neuroscience and developmental psychology into actionable solutions, while also advising on machine learning implementations.\n\n\n Collaborated closely with analysts across departments to design, build, and deploy various initiatives within the data platform, including the development, deployment, and maintenance of data services using R and PostgreSQL. Additionally, implemented best practices for continuous process automation, resulting in significant time savings for month-end processing.\n\n\n \n\n\n\n\n Conducted in-depth data analysis to classify customers using Bayesian inference, revealing distinct segments and patterns for data-driven decision-making and targeted strategies.\n\n\n Designed and implemented Apache NiFi ETL for efficient data processing and orchestration, contributing to streamlined data engineering workflows.\n\n\n Developed new internal tools for data analysts.\n\n\n \n\n\n\n\n Led the development of the data science infrastructure, creating an environment tailored for cutting-edge research and innovation.\n\n\n Developed and implemented complex statistical models in R to detect fraudulent transactions.\n\n\n Designed and maintained a data visualization dashboard in R Shiny, which allowed for easy monitoring of key performance indicators.\n\n\n \n\n\n\n\n Conducted lectures on statistics for the graduate program.\n\n\n Mentored and supervised graduate students in their statistical research projects, nurturing their analytical skills and research capabilities.\n\n\n Created and shared freely available data workshops in R and Python.\n\n\n\n\n\n\n\n Led an independent research project in Pr. Wraga’s visual cognition group.\n\n\n Designed and collected data for a cognitive neuroscience experiment.\n\n\n\n\n\n\n\n Led an independent research project as part of Dr. Burra’s attentional capture group.\n\n\n Conducted signal processing of EEG data."
  },
  {
    "objectID": "vitae.html#fa-cube-open-source-contributions",
    "href": "vitae.html#fa-cube-open-source-contributions",
    "title": "Full CV",
    "section": "",
    "text": "echarts4r\n\nR, JavaScript - 2023 - Present\nCollaborated on the development of echarts4r, a R package that expands the capabilities of R-based interactive visualizations with a powerful rendering engine, while enhancing flexibility and ease of use.\n\n\n\n\n Firebase\n\nR, JavaScript - 2023 - Present\nContributed to the firebase integration package for R, empowering the Shiny user community by enabling user authentication and secure file storage through Firebase Storage.\n\n\n\n\n DbVieweR\n\nR, SQL - 2022 - Present\nCreated a shiny app simulating a database management system featuring functions like login authentication, save/create/delete tables, add/rename columns, using either a PostgreSQL or SQLite back-end. DbVieweR\n\n\n\n\n brms\n\nR - 2022\nMade small contributions to the brms package, including resolving issues related to class definition.\n\n reprtree\n\nR - 2022\nIntegrated the caret ensemble for reprtree (implementation of representative trees from ensembles of tree-based machines).\n\n hBayesDM\n\nSTAN, R, Python - 2021\nCollaborated on the Hierarchical Bayesian modeling of Decision-Making tasks library. Built Q-learning algorithm for probabilistic selection task. hBayesDM\n\n 3dLMEr\n\nBASH, R - 2020\nCollaborated on the AFNI’s functions for 3Dimensional Linear Mixed-Effects Regression. Fixed residuals output image by adding bottom tolerance. 3dLMEr"
  },
  {
    "objectID": "vitae.html#fa-language-languages",
    "href": "vitae.html#fa-language-languages",
    "title": "Full CV",
    "section": "",
    "text": "French (Mother Tongue)\n Spanish (Mother Tongue)\n English (Fluent)\n German (Basic)\n Italian (Basic)"
  },
  {
    "objectID": "vitae.html#fa-trophy-accomplishments",
    "href": "vitae.html#fa-trophy-accomplishments",
    "title": "Full CV",
    "section": "",
    "text": "Top 2% on Stack Overflow in 2024\n Top 25% on GitHub in 2023\n Global Fellow scholarship recipient in 2017"
  },
  {
    "objectID": "vitae.html#fa-user-extracurricular-activities",
    "href": "vitae.html#fa-user-extracurricular-activities",
    "title": "Full CV",
    "section": "",
    "text": "Contributing to We Data’s mission by facilitating workshops, providing statistics tutorials, and participating in coding demonstrations.\n Organizing R-Lunches at UniGe.\n Co-founder of Go-Fast, a socially responsible Bike Messenger cooperative in Geneva.\n Co-president of La Rustine, a non-profit organization promoting non-motorized mobility and self-sustainability in Switzerland."
  },
  {
    "objectID": "vitae.html#fa-lightbulb-interests",
    "href": "vitae.html#fa-lightbulb-interests",
    "title": "Full CV",
    "section": "",
    "text": "Artificial Intelligence\n Machine Learning\n Bayesian Statistics\n Natural Language Processing\n Data Visualization\n Ethics in Technology\n Data Privacy\n Data Engineering\n Cybersecurity\n Data Journalism\n\n  References Available upon request."
  },
  {
    "objectID": "talks/2023-04-Deploy-your-R-code/index.html",
    "href": "talks/2023-04-Deploy-your-R-code/index.html",
    "title": "Deploy Your R Code!",
    "section": "",
    "text": "Deploy Your R Code!\n\n\n\n\n\nFull Slides\nPresentation Recording"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Blog\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nYour first chat bot with python!\n\n\n\n\n\n\n\nPython\n\n\nAI\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nDavid Munoz Tord\n\n\n\n\n\n\n  \n\n\n\n\nR Lunches in university of Geneva\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWe-Data Live on YouTube\n\n\n\n\n\n\n\nYouTube\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nData Manipulation with R\n\n\n\n\n\n\n\nR\n\n\nInteractive Exercises\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\nData Exploration with R\n\n\n\n\n\n\n\nR\n\n\nInteractive Exercises\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\nData Viz Fundamentals with R\n\n\n\n\n\n\n\nR\n\n\nInteractive Exercises\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services",
    "section": "",
    "text": "Services\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nData Engineering\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2022-12-echarts4r/index.html",
    "href": "projects/2022-12-echarts4r/index.html",
    "title": "Echarts4r",
    "section": "",
    "text": "Easy and beautiful interactive dataviz with Echarts4r!\nThis R package is a wrapper of the ECharts library. It provides a set of functions to generate interactive charts in R.\n\nlibrary(dplyr)\nlibrary(echarts4r)\nlibrary(lubridate)\n\n# Plot 1\nmtcars |&gt;\n  e_charts(cyl) |&gt; \n  e_boxplot(disp, colorBy=\"data\",) |&gt;\n  e_boxplot(hp, colorBy=\"data\",) |&gt;\n  e_boxplot(mpg, colorBy=\"data\",) |&gt;\n  e_tooltip(trigger = \"axis\") |&gt;\n  e_title(\"Descriptive Stats on mtcars Data\") |&gt;\n  e_toolbox_feature(feature = \"saveAsImage\")  # add the download button!\n\n\n\n\n# Plot 2\nlakers |&gt;           \n  mutate(date = lubridate::ymd(date)) |&gt;\n  mutate(points = points+runif(n=34624, min=-0.5, max=0.5) ) |&gt;\n  group_by(date) |&gt;\n  summarise(points2 = sum(points),\n            min= points2 - (50 +min(points)),\n            max=points2 + (50+max(points)) ) |&gt; \n  e_charts(date) |&gt;\n  e_line(points2) |&gt;\n  e_band2(min, max, color = \"lemonchiffon\") |&gt;\n  e_tooltip(trigger = \"axis\") |&gt;\n  e_title(\"Lakers Timeseries Data\") |&gt;\n  e_toolbox_feature(feature = \"saveAsImage\")  # add the download button!\n\n\n\n\n# Plot 3\nstarwars |&gt;\n  e_charts(mass) |&gt;\n  e_scatter(height, bind = name, symbol_size = 5, legend =F) |&gt;\n  e_datazoom() |&gt;\n  e_tooltip(\n    formatter = htmlwidgets::JS(\"\n      function(params){\n        return('&lt;strong&gt;' + params.name + \n                '&lt;/strong&gt;&lt;br /&gt;mass: ' + params.value[0] + \n                '&lt;br /&gt;height: ' + params.value[1]) \n                }\n    \")) |&gt; # little JS formatter to make it pretty\n  e_title(\"Starwars Outlier Data\") |&gt;\n  e_toolbox_feature(feature = \"saveAsImage\")  # add the download button!\n\n\n\n\n\n \n\n\n\n\n Full Screen\n\n\n\n\nDescription\n\n\n\nHighlights of this package:  - Provide functions to generate interactive charts in R. - The package is built on top of the ECharts library, which is a powerful library for data visualization. - The package provides a set of functions that can be used to generate charts with ECharts. - The package is still under development. More features will be added in the future. - The package is open source.\n Features of the package: \n\nProvides more than 20 chart types available out of the box, along with a dozen components, and each of them can be arbitrarily combined to use.\nHas a powerful rendering engine that allows you to easily switch between Canvas and SVG rendering. Progressive rendering and stream loading make it possible to render 10 million data in realtime.\nOffers professional data analysis through datasets, which support data transforms like filtering, clustering, and regression to help analyze multi-dimensional analysis of the same data.\nHas an elegant visual design that follows visualization principles and supports responsive design. Flexible configurations make it easy to customize.\nHas a healthy community that ensures the healthy development of the project and contributes a wealth of third-party extensions.\nIs accessibility-friendly with automatically generated chart descriptions and decal patterns that help users with disabilities understand the content and the stories behind the charts.\n\n\nYou can check the project on Github."
  },
  {
    "objectID": "projects/DbVieweR/index.html",
    "href": "projects/DbVieweR/index.html",
    "title": "DbVieweR",
    "section": "",
    "text": "DbVieweR an R package for database management\nDbVieweR is a Shiny app that simulates a database management system, featuring functions like login/logout, save/create/delete tables, and add/rename columns.\n\n\n\n\n Demo \n\n\nDescription\n\n\n\nHighlights of this app:  - Back-end database: Utilizes SQLite or PostgreSQL for storing dummy data. - Authorization: Incorporates the shinyauthr package to add an authentication layer to the app.  Features of the app:  - Save tables: Store sales summary tables in the database. - Update existing tables: Rename tables or columns. - Create new tables: Customize table and column names, with options for integer, float, varchar(255), and boolean columns. - Create entries: Add entries to tables with customizable column types. - Delete tables: Accessible only with specific authorization. - Robustness: Defense mechanism prevents duplicates, invalid expressions, and conflicts with SQL keywords.  Check out the project on GitHub."
  }
]